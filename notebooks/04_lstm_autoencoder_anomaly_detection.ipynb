{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder による時系列異常検知\n",
    "\n",
    "このノートブックでは、**LSTM Autoencoder**を使った時系列データの異常検知を学びます。\n",
    "\n",
    "## LSTM Autoencoderとは？\n",
    "\n",
    "**LSTM Autoencoder**は、時系列データの正常なパターンを学習し、そのパターンから外れたデータを異常として検出する深層学習の手法です。\n",
    "\n",
    "### なぜ時系列異常検知に有効なのか？\n",
    "\n",
    "1. **時間的な文脈を考慮**: 過去の値の連鎖パターンを学習\n",
    "2. **複雑なパターンを学習**: 非線形なパターンも捉えられる\n",
    "3. **教師なし学習**: 正常データのみで学習可能\n",
    "4. **柔軟性**: 多変量時系列にも対応\n",
    "\n",
    "### Isolation Forestとの違い\n",
    "\n",
    "| 特徴 | Isolation Forest | LSTM Autoencoder |\n",
    "|------|------------------|------------------|\n",
    "| データ | 静的データ | 時系列データ |\n",
    "| 学習内容 | 異常を隔離 | 正常パターンを再構成 |\n",
    "| 時間的依存性 | 考慮しない | **考慮する** |\n",
    "| 計算速度 | 高速 | やや遅い |\n",
    "| 複雑なパターン | シンプル | 複雑なパターンも学習 |\n",
    "\n",
    "### サイバー攻撃検知での応用\n",
    "\n",
    "- **DDoS攻撃**: トラフィックの急激な増加パターン\n",
    "- **ポートスキャン**: 短時間での連続アクセスパターン\n",
    "- **データ漏洩**: 通常と異なるデータ転送パターン\n",
    "- **異常ログイン**: 時間帯・頻度の異常パターン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     11\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mKERAS_BACKEND\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# NumPyバックエンドを使用（軽量）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models, callbacks\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# scikit-learn（前処理・評価）\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _tf_keras \u001b[38;5;28;01mas\u001b[39;00m _tf_keras\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/_tf_keras/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/_tf_keras/keras/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m callbacks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/activations/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize \u001b[38;5;28;01mas\u001b[39;00m deserialize\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get \u001b[38;5;28;01mas\u001b[39;00m get\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialize \u001b[38;5;28;01mas\u001b[39;00m serialize\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/src/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/src/activations/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m celu\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exponential\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/src/activations/activations.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/src/backend/__init__.py:49\u001b[39m\n\u001b[32m     47\u001b[39m     distribution_lib = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable \u001b[38;5;28;01mas\u001b[39;00m BackendVariable\n\u001b[32m     52\u001b[39m     distribution_lib = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/src/backend/numpy/__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/src/backend/numpy/math.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fft \u001b[38;5;28;01mas\u001b[39;00m jax_fft\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fft2 \u001b[38;5;28;01mas\u001b[39;00m jax_fft2\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_to_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/src/backend/jax/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_nnx_enabled\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribution_lib\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gh/manaphy/kaggle-workspace/.venv/lib/python3.13/site-packages/keras/src/backend/jax/core.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax_sparse\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "# データ処理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Keras (深層学習)\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'numpy'  # NumPyバックエンドを使用（軽量）\n",
    "\n",
    "import keras\n",
    "from keras import layers, models, callbacks\n",
    "\n",
    "# scikit-learn（前処理・評価）\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 設定\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Keras backend: {keras.backend.backend()}\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Autoencoderの理論\n",
    "\n",
    "### 2.1 Autoencoderとは？\n",
    "\n",
    "**Autoencoder**は、入力データを圧縮（エンコード）し、再び元の形に復元（デコード）するニューラルネットワークです。\n",
    "\n",
    "```\n",
    "入力データ → [Encoder] → 潜在表現 → [Decoder] → 復元データ\n",
    "```\n",
    "\n",
    "#### 学習の目的\n",
    "- **入力と出力がなるべく同じになるように学習**\n",
    "- 正常データで学習すると、**正常パターンを再構成できる**\n",
    "- 異常データは**再構成誤差が大きくなる**\n",
    "\n",
    "### 2.2 LSTM (Long Short-Term Memory)\n",
    "\n",
    "**LSTM**は、時系列データの長期的な依存関係を学習できるリカレントニューラルネットワーク（RNN）の一種です。\n",
    "\n",
    "#### LSTMの特徴\n",
    "- **記憶セル**: 過去の情報を保持\n",
    "- **ゲート機構**: 情報の流れを制御\n",
    "  - **入力ゲート**: 新しい情報をどれだけ記憶するか\n",
    "  - **忘却ゲート**: 古い情報をどれだけ忘れるか\n",
    "  - **出力ゲート**: 記憶からどれだけ出力するか\n",
    "\n",
    "### 2.3 LSTM Autoencoderの構造\n",
    "\n",
    "```\n",
    "時系列データ (例: [t1, t2, t3, t4, t5])\n",
    "    ↓\n",
    "[LSTM Encoder]\n",
    "    - 時系列を圧縮して潜在表現を作成\n",
    "    - 最後の隠れ状態がエンコード結果\n",
    "    ↓\n",
    "潜在表現 (圧縮された特徴)\n",
    "    ↓\n",
    "[RepeatVector]\n",
    "    - 潜在表現を時系列の長さ分繰り返す\n",
    "    ↓\n",
    "[LSTM Decoder]\n",
    "    - 潜在表現から元の時系列を復元\n",
    "    ↓\n",
    "復元された時系列 (例: [t1', t2', t3', t4', t5'])\n",
    "```\n",
    "\n",
    "### 2.4 異常検知の仕組み\n",
    "\n",
    "1. **正常データで学習**: 正常なパターンを再構成する能力を獲得\n",
    "2. **再構成誤差を計算**: 入力と出力の差（MSE、MAEなど）\n",
    "3. **閾値設定**: 再構成誤差がある閾値を超えたら異常と判定\n",
    "\n",
    "```python\n",
    "再構成誤差 = MSE(入力, 復元データ)\n",
    "\n",
    "if 再構成誤差 > 閾値:\n",
    "    異常\n",
    "else:\n",
    "    正常\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. シンプルな例: 正弦波の異常検知\n",
    "\n",
    "まず、基礎を理解するためにシンプルな正弦波データで異常検知を実装します。\n",
    "\n",
    "### データの生成\n",
    "- **正常データ**: 滑らかな正弦波\n",
    "- **異常データ**: スパイク（急激な値の変化）を含む波形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正弦波データの生成\n",
    "def generate_sine_wave(n_samples=1000, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    正弦波データを生成\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: サンプル数\n",
    "    - noise_level: ノイズの強度\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 50, n_samples)\n",
    "    y = np.sin(x) + np.random.normal(0, noise_level, n_samples)\n",
    "    return y\n",
    "\n",
    "# 異常データの生成（スパイクを追加）\n",
    "def add_anomalies(data, n_anomalies=5, spike_magnitude=3):\n",
    "    \"\"\"\n",
    "    データに異常（スパイク）を追加\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 元のデータ\n",
    "    - n_anomalies: 異常の数\n",
    "    - spike_magnitude: スパイクの大きさ\n",
    "    \"\"\"\n",
    "    data_with_anomalies = data.copy()\n",
    "    anomaly_indices = np.random.choice(len(data), n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        data_with_anomalies[idx] += spike_magnitude * np.random.choice([-1, 1])\n",
    "    \n",
    "    return data_with_anomalies, anomaly_indices\n",
    "\n",
    "# データ生成\n",
    "normal_data = generate_sine_wave(n_samples=1000, noise_level=0.1)\n",
    "test_data, anomaly_indices = add_anomalies(normal_data, n_anomalies=10, spike_magnitude=3)\n",
    "\n",
    "print(f\"正常データのサイズ: {normal_data.shape}\")\n",
    "print(f\"テストデータのサイズ: {test_data.shape}\")\n",
    "print(f\"異常の位置: {sorted(anomaly_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの可視化\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# 正常データ\n",
    "axes[0].plot(normal_data, label='Normal Data', color='blue', alpha=0.7)\n",
    "axes[0].set_title('Normal Sine Wave', fontsize=14)\n",
    "axes[0].set_xlabel('Time Steps')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 異常データ\n",
    "axes[1].plot(test_data, label='Test Data', color='blue', alpha=0.7)\n",
    "axes[1].scatter(anomaly_indices, test_data[anomaly_indices], \n",
    "                color='red', s=100, label='Anomalies', zorder=5)\n",
    "axes[1].set_title('Sine Wave with Anomalies (Spikes)', fontsize=14)\n",
    "axes[1].set_xlabel('Time Steps')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 時系列データの準備\n",
    "\n",
    "LSTM Autoencoderに入力するため、時系列データを**ウィンドウ**に分割します。\n",
    "\n",
    "#### ウィンドウとは？\n",
    "- 連続したデータポイントの塊\n",
    "- 例: `window_size=10` の場合、[t1, t2, ..., t10]が1つのサンプル\n",
    "\n",
    "```\n",
    "元データ: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "window_size=5\n",
    "\n",
    "サンプル1: [1, 2, 3, 4, 5]\n",
    "サンプル2: [2, 3, 4, 5, 6]\n",
    "サンプル3: [3, 4, 5, 6, 7]\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size):\n",
    "    \"\"\"\n",
    "    時系列データをウィンドウに分割\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 1次元の時系列データ\n",
    "    - window_size: ウィンドウのサイズ\n",
    "    \n",
    "    Returns:\n",
    "    - sequences: (サンプル数, window_size, 1)の3次元配列\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        seq = data[i:i + window_size]\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    # LSTMの入力形式: (サンプル数, タイムステップ, 特徴量数)\n",
    "    sequences = sequences.reshape((sequences.shape[0], sequences.shape[1], 1))\n",
    "    return sequences\n",
    "\n",
    "# ウィンドウサイズの設定\n",
    "WINDOW_SIZE = 50  # 50ステップを1つのシーケンスとして扱う\n",
    "\n",
    "# 正常データをウィンドウに分割（訓練用）\n",
    "X_train = create_sequences(normal_data, WINDOW_SIZE)\n",
    "\n",
    "# テストデータをウィンドウに分割\n",
    "X_test = create_sequences(test_data, WINDOW_SIZE)\n",
    "\n",
    "print(f\"訓練データの形状: {X_train.shape}\")\n",
    "print(f\"テストデータの形状: {X_test.shape}\")\n",
    "print(f\"\\n形状の意味: (サンプル数={X_train.shape[0]}, ウィンドウサイズ={X_train.shape[1]}, 特徴量数={X_train.shape[2]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの正規化（0-1の範囲にスケーリング）\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 訓練データでfitしてtransform\n",
    "X_train_scaled = scaler.fit_transform(X_train\n",
    "                                      .reshape(-1, 1)).reshape(X_train.shape)\n",
    "\n",
    "# テストデータはtransformのみ（訓練データの統計量を使用）\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "print(f\"訓練データの範囲: [{X_train_scaled.min():.4f}, {X_train_scaled.max():.4f}]\")\n",
    "print(f\"テストデータの範囲: [{X_test_scaled.min():.4f}, {X_test_scaled.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LSTM Autoencoderモデルの構築\n",
    "\n",
    "Kerasを使ってLSTM Autoencoderを実装します。\n",
    "\n",
    "#### モデルの構造\n",
    "\n",
    "```\n",
    "Input: (window_size, 1)\n",
    "    ↓\n",
    "LSTM(128) - Encoder\n",
    "    ↓\n",
    "LSTM(64) - Encoder\n",
    "    ↓\n",
    "LSTM(32) - Encoder (潜在表現)\n",
    "    ↓\n",
    "RepeatVector(window_size) - 潜在表現を繰り返す\n",
    "    ↓\n",
    "LSTM(32, return_sequences=True) - Decoder\n",
    "    ↓\n",
    "LSTM(64, return_sequences=True) - Decoder\n",
    "    ↓\n",
    "LSTM(128, return_sequences=True) - Decoder\n",
    "    ↓\n",
    "TimeDistributed(Dense(1)) - 各タイムステップで出力\n",
    "    ↓\n",
    "Output: (window_size, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_autoencoder(window_size, n_features=1):\n",
    "    \"\"\"\n",
    "    LSTM Autoencoderモデルを構築\n",
    "    \n",
    "    Parameters:\n",
    "    - window_size: ウィンドウサイズ（タイムステップ数）\n",
    "    - n_features: 特徴量の数\n",
    "    \n",
    "    Returns:\n",
    "    - model: Kerasモデル\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Encoder\n",
    "        layers.Input(shape=(window_size, n_features)),\n",
    "        layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        layers.LSTM(64, activation='relu', return_sequences=True),\n",
    "        layers.LSTM(32, activation='relu', return_sequences=False),  # 潜在表現\n",
    "        \n",
    "        # 潜在表現を繰り返す\n",
    "        layers.RepeatVector(window_size),\n",
    "        \n",
    "        # Decoder\n",
    "        layers.LSTM(32, activation='relu', return_sequences=True),\n",
    "        layers.LSTM(64, activation='relu', return_sequences=True),\n",
    "        layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        \n",
    "        # 出力層（各タイムステップで1つの値を出力）\n",
    "        layers.TimeDistributed(layers.Dense(n_features))\n",
    "    ], name='LSTM_Autoencoder')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# モデルの構築\n",
    "model = build_lstm_autoencoder(window_size=WINDOW_SIZE, n_features=1)\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',  # 平均二乗誤差\n",
    "    metrics=['mae']  # 平均絶対誤差\n",
    ")\n",
    "\n",
    "# モデルのサマリーを表示\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 モデルの訓練\n",
    "\n",
    "正常データのみを使ってモデルを訓練します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping（過学習を防ぐ）\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# モデルの訓練\n",
    "# 注: 入力と出力が同じ（Autoencoderの特徴）\n",
    "history = model.fit(\n",
    "    X_train_scaled, X_train_scaled,  # 入力 = 出力\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n訓練完了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練の履歴を可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Training and Validation MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 異常検知の実行\n",
    "\n",
    "訓練したモデルを使ってテストデータの異常を検出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの再構成\n",
    "X_test_reconstructed = model.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "# 再構成誤差を計算（各サンプルごとのMSE）\n",
    "reconstruction_errors = np.mean(np.square(X_test_scaled - X_test_reconstructed), axis=(1, 2))\n",
    "\n",
    "print(f\"再構成誤差の統計:\")\n",
    "print(f\"  平均: {reconstruction_errors.mean():.6f}\")\n",
    "print(f\"  標準偏差: {reconstruction_errors.std():.6f}\")\n",
    "print(f\"  最小値: {reconstruction_errors.min():.6f}\")\n",
    "print(f\"  最大値: {reconstruction_errors.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再構成誤差の分布を可視化\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.hist(reconstruction_errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 閾値の設定（平均 + 3σ）\n",
    "threshold = reconstruction_errors.mean() + 3 * reconstruction_errors.std()\n",
    "\n",
    "print(f\"異常検知の閾値: {threshold:.6f}\")\n",
    "\n",
    "# 異常の判定\n",
    "anomalies_detected = reconstruction_errors > threshold\n",
    "\n",
    "print(f\"\\n検出された異常数: {anomalies_detected.sum()}\")\n",
    "print(f\"異常の割合: {anomalies_detected.sum() / len(anomalies_detected) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の可視化\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# 元のテストデータと真の異常\n",
    "axes[0].plot(test_data, label='Test Data', color='blue', alpha=0.7)\n",
    "axes[0].scatter(anomaly_indices, test_data[anomaly_indices], \n",
    "                color='red', s=100, label='True Anomalies', zorder=5)\n",
    "axes[0].set_title('Test Data with True Anomalies', fontsize=14)\n",
    "axes[0].set_xlabel('Time Steps')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 再構成誤差と検出された異常\n",
    "axes[1].plot(reconstruction_errors, label='Reconstruction Error', color='green', alpha=0.7)\n",
    "axes[1].axhline(threshold, color='red', linestyle='--', label=f'Threshold ({threshold:.4f})')\n",
    "axes[1].scatter(np.where(anomalies_detected)[0], \n",
    "                reconstruction_errors[anomalies_detected],\n",
    "                color='red', s=100, label='Detected Anomalies', zorder=5)\n",
    "axes[1].set_title('Reconstruction Error and Detected Anomalies', fontsize=14)\n",
    "axes[1].set_xlabel('Window Index')\n",
    "axes[1].set_ylabel('Reconstruction Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検出精度の評価\n",
    "# ウィンドウごとに真の異常が含まれているかをチェック\n",
    "true_labels = np.zeros(len(X_test), dtype=bool)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    window_range = range(i, i + WINDOW_SIZE)\n",
    "    if any(idx in window_range for idx in anomaly_indices):\n",
    "        true_labels[i] = True\n",
    "\n",
    "# 混同行列の計算\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(true_labels, anomalies_detected)\n",
    "\n",
    "print(\"混同行列:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, anomalies_detected, \n",
    "                            target_names=['Normal', 'Anomaly']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 再構成の可視化\n",
    "\n",
    "モデルがどのようにデータを再構成しているかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# いくつかのサンプルの再構成を可視化\n",
    "n_samples_to_plot = 3\n",
    "sample_indices = [0, len(X_test) // 2, len(X_test) - 1]\n",
    "\n",
    "fig, axes = plt.subplots(n_samples_to_plot, 1, figsize=(14, 10))\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    original = X_test_scaled[sample_idx].flatten()\n",
    "    reconstructed = X_test_reconstructed[sample_idx].flatten()\n",
    "    error = reconstruction_errors[sample_idx]\n",
    "    is_anomaly = anomalies_detected[sample_idx]\n",
    "    \n",
    "    axes[idx].plot(original, label='Original', color='blue', linewidth=2)\n",
    "    axes[idx].plot(reconstructed, label='Reconstructed', color='orange', linewidth=2, linestyle='--')\n",
    "    \n",
    "    title = f'Sample {sample_idx} - Error: {error:.6f}'\n",
    "    if is_anomaly:\n",
    "        title += ' [ANOMALY DETECTED]'\n",
    "        axes[idx].set_facecolor('#ffcccc')\n",
    "    \n",
    "    axes[idx].set_title(title, fontsize=12)\n",
    "    axes[idx].set_xlabel('Time Step')\n",
    "    axes[idx].set_ylabel('Normalized Value')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Isolation Forestとの比較\n",
    "\n",
    "同じデータでIsolation Forestを使った場合と比較してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forestによる異常検知\n",
    "# ウィンドウデータを2次元に変換（Isolation Forestは時系列を考慮しない）\n",
    "X_test_flat = X_test_scaled.reshape(X_test_scaled.shape[0], -1)\n",
    "\n",
    "# モデルの訓練と予測\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.05,  # 異常の割合\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_flat = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "iso_forest.fit(X_train_flat)\n",
    "\n",
    "# 予測（-1: 異常, 1: 正常）\n",
    "iso_predictions = iso_forest.predict(X_test_flat)\n",
    "iso_anomalies = iso_predictions == -1\n",
    "\n",
    "print(f\"Isolation Forestで検出された異常数: {iso_anomalies.sum()}\")\n",
    "print(f\"異常の割合: {iso_anomalies.sum() / len(iso_anomalies) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つの手法の比較\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# 元のデータと真の異常\n",
    "axes[0].plot(test_data, label='Test Data', color='blue', alpha=0.7)\n",
    "axes[0].scatter(anomaly_indices, test_data[anomaly_indices], \n",
    "                color='red', s=100, label='True Anomalies', zorder=5)\n",
    "axes[0].set_title('True Anomalies', fontsize=14)\n",
    "axes[0].set_xlabel('Time Steps')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# LSTM Autoencoderの結果\n",
    "axes[1].plot(test_data, label='Test Data', color='blue', alpha=0.3)\n",
    "detected_indices_lstm = np.where(anomalies_detected)[0]\n",
    "for idx in detected_indices_lstm:\n",
    "    axes[1].axvspan(idx, idx + WINDOW_SIZE, alpha=0.3, color='red')\n",
    "axes[1].scatter(anomaly_indices, test_data[anomaly_indices], \n",
    "                color='red', s=100, label='True Anomalies', zorder=5)\n",
    "axes[1].set_title('LSTM Autoencoder Detection', fontsize=14)\n",
    "axes[1].set_xlabel('Time Steps')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Isolation Forestの結果\n",
    "axes[2].plot(test_data, label='Test Data', color='blue', alpha=0.3)\n",
    "detected_indices_iso = np.where(iso_anomalies)[0]\n",
    "for idx in detected_indices_iso:\n",
    "    axes[2].axvspan(idx, idx + WINDOW_SIZE, alpha=0.3, color='orange')\n",
    "axes[2].scatter(anomaly_indices, test_data[anomaly_indices], \n",
    "                color='red', s=100, label='True Anomalies', zorder=5)\n",
    "axes[2].set_title('Isolation Forest Detection', fontsize=14)\n",
    "axes[2].set_xlabel('Time Steps')\n",
    "axes[2].set_ylabel('Value')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価指標の比較\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# LSTM Autoencoder\n",
    "lstm_precision = precision_score(true_labels, anomalies_detected, zero_division=0)\n",
    "lstm_recall = recall_score(true_labels, anomalies_detected, zero_division=0)\n",
    "lstm_f1 = f1_score(true_labels, anomalies_detected, zero_division=0)\n",
    "\n",
    "# Isolation Forest\n",
    "iso_precision = precision_score(true_labels, iso_anomalies, zero_division=0)\n",
    "iso_recall = recall_score(true_labels, iso_anomalies, zero_division=0)\n",
    "iso_f1 = f1_score(true_labels, iso_anomalies, zero_division=0)\n",
    "\n",
    "# 結果の表示\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['LSTM Autoencoder', 'Isolation Forest'],\n",
    "    'Precision': [lstm_precision, iso_precision],\n",
    "    'Recall': [lstm_recall, iso_recall],\n",
    "    'F1-Score': [lstm_f1, iso_f1]\n",
    "})\n",
    "\n",
    "print(\"\\n評価指標の比較:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, comparison_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, comparison_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, comparison_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparison of LSTM Autoencoder vs Isolation Forest')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Method'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. まとめ\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "#### 1. LSTM Autoencoderの基礎\n",
    "- **Autoencoder**: 入力を圧縮して復元するニューラルネットワーク\n",
    "- **LSTM**: 時系列の長期的な依存関係を学習\n",
    "- **再構成誤差**: 正常パターンから外れたデータは誤差が大きくなる\n",
    "\n",
    "#### 2. 実装のポイント\n",
    "- **ウィンドウ分割**: 時系列データをLSTMに入力するための前処理\n",
    "- **正規化**: データを0-1の範囲にスケーリング\n",
    "- **閾値設定**: 平均 + 3σなど、統計的な基準で異常を判定\n",
    "\n",
    "#### 3. Isolation Forestとの比較\n",
    "\n",
    "| 特徴 | LSTM Autoencoder | Isolation Forest |\n",
    "|------|------------------|------------------|\n",
    "| **時系列の考慮** | ✅ あり | ❌ なし |\n",
    "| **学習内容** | 正常パターンの再構成 | データポイントの隔離 |\n",
    "| **計算速度** | 遅い（訓練に時間） | 速い |\n",
    "| **精度** | 時系列パターンに強い | 静的データに強い |\n",
    "| **解釈性** | やや複雑 | シンプル |\n",
    "| **適用場面** | 時系列データの異常検知 | 一般的な異常検知 |\n",
    "\n",
    "### 実務での活用\n",
    "\n",
    "#### DDoS攻撃検知への応用\n",
    "```python\n",
    "時系列特徴量:\n",
    "- リクエスト数の時系列\n",
    "- パケットサイズの時系列\n",
    "- 送信元IPの多様性の時系列\n",
    "- レスポンス時間の時系列\n",
    "\n",
    "→ これらをLSTM Autoencoderで学習\n",
    "→ 正常な通信パターンを学習\n",
    "→ 異常なトラフィックパターンを検出\n",
    "```\n",
    "\n",
    "#### その他の応用例\n",
    "- **システム監視**: CPUやメモリ使用率の異常検知\n",
    "- **製造業**: センサーデータからの設備異常検知\n",
    "- **金融**: 取引パターンの異常検知\n",
    "- **医療**: 心電図などの生体信号の異常検知\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "1. **実践的なデータセットで試す**\n",
    "   - KDD Cup 99、NSL-KDD（ネットワーク侵入検知）\n",
    "   - CIC-IDS2017/2018（最新のサイバー攻撃データ）\n",
    "   - UNSW-NB15（オーストラリアのネットワークデータ）\n",
    "\n",
    "2. **モデルの改善**\n",
    "   - **Bidirectional LSTM**: 双方向LSTM（前後の文脈を考慮）\n",
    "   - **Variational Autoencoder (VAE)**: 確率的なAutoencoder\n",
    "   - **Attention機構**: 重要な時間ステップに注目\n",
    "\n",
    "3. **多変量時系列への拡張**\n",
    "   - 複数の特徴量を同時に扱う\n",
    "   - 特徴量間の相関も学習\n",
    "\n",
    "4. **リアルタイム検知システムの構築**\n",
    "   - ストリーミングデータへの対応\n",
    "   - オンライン学習\n",
    "   - アラートシステムとの統合\n",
    "\n",
    "### 参考資料\n",
    "\n",
    "- [Keras公式ドキュメント](https://keras.io/)\n",
    "- [LSTM Autoencoder for Anomaly Detection](https://arxiv.org/abs/1607.00148)\n",
    "- [Time Series Anomaly Detection Tutorial](https://keras.io/examples/timeseries/timeseries_anomaly_detection/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
