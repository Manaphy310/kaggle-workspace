{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教師なし学習: 異常検知（Anomaly Detection）\n",
    "\n",
    "このノートブックでは、タイタニックデータセットを使って**異常検知（Anomaly Detection）**の手法を学びます。\n",
    "\n",
    "## 異常検知とは？\n",
    "\n",
    "**異常検知**は、正常なデータパターンから外れた「異常値」や「外れ値」を検出する教師なし学習の手法です。\n",
    "\n",
    "### 実務での応用例\n",
    "\n",
    "1. **不正検知**: クレジットカード不正利用、マネーロンダリング\n",
    "2. **システム監視**: サーバーの異常動作、ネットワーク侵入検知\n",
    "3. **製造業**: 不良品検出、機器の故障予兆\n",
    "4. **医療**: 異常な検査値、珍しい疾患パターン\n",
    "5. **データクレンジング**: データの品質チェック、入力ミス検出\n",
    "\n",
    "### タイタニックデータでの異常検知の目的\n",
    "\n",
    "- 通常とは異なる特徴を持つ乗客を発見する\n",
    "- データの品質をチェックする（入力ミス、外れ値など）\n",
    "- 異常値を除外・処理して予測モデルの精度を向上させる\n",
    "\n",
    "## 学習する手法\n",
    "\n",
    "1. **Isolation Forest**: ランダムに分割して異常を隔離\n",
    "2. **One-Class SVM**: サポートベクターマシンで正常領域を学習\n",
    "3. **Local Outlier Factor (LOF)**: 局所密度から異常を判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ処理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 機械学習（異常検知）\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# 前処理・評価\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 設定\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train = pd.read_csv('../data/raw/train.csv')\n",
    "test = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "print(f'訓練データサイズ: {train.shape}')\n",
    "print(f'テストデータサイズ: {test.shape}')\n",
    "\n",
    "# 最初の数行を確認\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データの前処理\n",
    "\n",
    "異常検知を行うために、データを数値化して前処理します。\n",
    "\n",
    "### 前処理の手順\n",
    "\n",
    "1. **必要な特徴量を選択**\n",
    "2. **カテゴリ変数を数値化**（Label Encoding）\n",
    "3. **欠損値を補完**\n",
    "4. **特徴量を標準化**（平均0、分散1に変換）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのコピーを作成（元データを保持）\n",
    "df = train.copy()\n",
    "\n",
    "# 特徴量エンジニアリング\n",
    "# 家族サイズ\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "# 単身かどうか\n",
    "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# 敬称（Title）を抽出\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# 敬称を整理\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n",
    "    'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',\n",
    "    'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',\n",
    "    'Capt': 'Rare', 'Sir': 'Rare'\n",
    "}\n",
    "df['Title'] = df['Title'].map(title_mapping)\n",
    "\n",
    "# 使用する特徴量を選択\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize', 'IsAlone', 'Title']\n",
    "\n",
    "# 特徴量のみを抽出\n",
    "X = df[features].copy()\n",
    "\n",
    "print(\"前処理前のデータ:\")\n",
    "print(X.head())\n",
    "print(f\"\\n欠損値の数:\\n{X.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリ変数を数値化\n",
    "categorical_features = ['Sex', 'Embarked', 'Title']\n",
    "\n",
    "for col in categorical_features:\n",
    "    # 欠損値を最頻値で補完してからエンコード\n",
    "    X[col].fillna(X[col].mode()[0], inplace=True)\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "print(\"カテゴリ変数エンコード後:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値変数の欠損値を中央値で補完\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# DataFrame形式に戻す\n",
    "X_processed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "print(\"欠損値補完後:\")\n",
    "print(f\"欠損値の数: {X_processed.isnull().sum().sum()}\")\n",
    "print(X_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化（平均0、分散1に変換）\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_processed)\n",
    "\n",
    "# DataFrame形式に戻す\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_processed.columns)\n",
    "\n",
    "print(\"標準化後のデータ:\")\n",
    "print(X_scaled_df.head())\n",
    "print(f\"\\n平均: {X_scaled_df.mean().round(2)}\")\n",
    "print(f\"\\n標準偏差: {X_scaled_df.std().round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 手法1: Isolation Forest\n",
    "\n",
    "### Isolation Forestとは？\n",
    "\n",
    "**Isolation Forest**は、異常値を「隔離しやすい」という性質を利用した手法です。\n",
    "\n",
    "#### 基本的な考え方\n",
    "\n",
    "1. **正常データ**: 密集しているため、ランダムに分割すると隔離するのに多くの分割が必要\n",
    "2. **異常データ**: 離れているため、少ない分割で隔離できる\n",
    "\n",
    "#### 仕組み\n",
    "\n",
    "1. ランダムに特徴量を選択\n",
    "2. ランダムに分割点を選んでデータを分割\n",
    "3. これを繰り返して木構造（ツリー）を作成\n",
    "4. **分割回数が少ないデータ = 異常**と判定\n",
    "\n",
    "#### 主なパラメータ\n",
    "\n",
    "- **n_estimators**: 作成する木の数（デフォルト: 100）\n",
    "- **contamination**: 異常データの割合の推定値（デフォルト: 0.1 = 10%）\n",
    "- **max_samples**: 各木で使用するサンプル数\n",
    "- **random_state**: 乱数シード（再現性のため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forestモデルの作成\n",
    "# contamination: 異常と判定するデータの割合（5%と仮定）\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # 全CPUコアを使用\n",
    ")\n",
    "\n",
    "# 学習と予測\n",
    "# 予測結果: 1 = 正常, -1 = 異常\n",
    "iso_predictions = iso_forest.fit_predict(X_scaled)\n",
    "\n",
    "# 異常スコアを計算（値が小さいほど異常）\n",
    "iso_scores = iso_forest.score_samples(X_scaled)\n",
    "\n",
    "# 結果を元のDataFrameに追加\n",
    "df['iso_prediction'] = iso_predictions\n",
    "df['iso_score'] = iso_scores\n",
    "\n",
    "print(f\"異常と判定されたデータ数: {(iso_predictions == -1).sum()}件\")\n",
    "print(f\"正常と判定されたデータ数: {(iso_predictions == 1).sum()}件\")\n",
    "print(f\"異常データの割合: {(iso_predictions == -1).sum() / len(iso_predictions) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forestの結果を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異常スコアの分布を可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ヒストグラム\n",
    "axes[0].hist(df['iso_score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(df[df['iso_prediction'] == -1]['iso_score'].max(), \n",
    "                color='red', linestyle='--', label='異常の閾値')\n",
    "axes[0].set_xlabel('Anomaly Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Isolation Forest Anomaly Scores')\n",
    "axes[0].legend()\n",
    "\n",
    "# ボックスプロット（正常 vs 異常）\n",
    "df_plot = df.copy()\n",
    "df_plot['Label'] = df_plot['iso_prediction'].map({1: 'Normal', -1: 'Anomaly'})\n",
    "sns.boxplot(data=df_plot, x='Label', y='iso_score', ax=axes[1])\n",
    "axes[1].set_title('Anomaly Score: Normal vs Anomaly')\n",
    "axes[1].set_ylabel('Anomaly Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAで2次元に削減して可視化\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                     c=iso_predictions, \n",
    "                     cmap='coolwarm', \n",
    "                     alpha=0.6,\n",
    "                     edgecolors='black',\n",
    "                     linewidths=0.5)\n",
    "plt.colorbar(scatter, label='Prediction (1=Normal, -1=Anomaly)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title('Isolation Forest: Normal vs Anomaly (PCA Visualization)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCAによる説明分散比率: PC1={pca.explained_variance_ratio_[0]*100:.2f}%, PC2={pca.explained_variance_ratio_[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 異常と判定されたデータを分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異常データを抽出\n",
    "anomalies_iso = df[df['iso_prediction'] == -1].copy()\n",
    "\n",
    "print(\"異常と判定されたデータ（上位10件）:\")\n",
    "print(anomalies_iso[['PassengerId', 'Name', 'Pclass', 'Sex', 'Age', 'Fare', \n",
    "                      'FamilySize', 'Survived', 'iso_score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正常データと異常データの特徴を比較\n",
    "normal_iso = df[df['iso_prediction'] == 1]\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Normal_Mean': normal_iso[['Pclass', 'Age', 'Fare', 'FamilySize', 'Survived']].mean(),\n",
    "    'Anomaly_Mean': anomalies_iso[['Pclass', 'Age', 'Fare', 'FamilySize', 'Survived']].mean()\n",
    "})\n",
    "comparison['Difference'] = comparison['Anomaly_Mean'] - comparison['Normal_Mean']\n",
    "\n",
    "print(\"正常データと異常データの平均値比較:\")\n",
    "print(comparison.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量ごとの分布比較\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "features_to_plot = ['Age', 'Fare', 'FamilySize', 'Pclass']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    ax.hist([normal_iso[feature].dropna(), anomalies_iso[feature].dropna()], \n",
    "            bins=30, alpha=0.7, label=['Normal', 'Anomaly'])\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{feature} Distribution: Normal vs Anomaly')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 手法2: One-Class SVM\n",
    "\n",
    "### One-Class SVMとは？\n",
    "\n",
    "**One-Class SVM**は、正常データの領域を学習し、その領域から外れたデータを異常と判定する手法です。\n",
    "\n",
    "#### 基本的な考え方\n",
    "\n",
    "1. 正常データを囲む**決定境界**を学習\n",
    "2. この境界の外側にあるデータを異常と判定\n",
    "3. カーネル関数を使って非線形な境界も学習可能\n",
    "\n",
    "#### 主なパラメータ\n",
    "\n",
    "- **nu**: 異常データの上限割合（0 < nu ≤ 1）。小さいほど厳しく判定\n",
    "- **kernel**: カーネル関数（'rbf', 'linear', 'poly'など）\n",
    "  - 'rbf': ガウシアンカーネル（デフォルト、非線形）\n",
    "  - 'linear': 線形カーネル\n",
    "- **gamma**: RBFカーネルのパラメータ（'scale'または数値）\n",
    "\n",
    "#### Isolation Forestとの違い\n",
    "\n",
    "| 項目 | Isolation Forest | One-Class SVM |\n",
    "|------|------------------|---------------|\n",
    "| アプローチ | 異常を隔離しやすいか | 正常領域を学習 |\n",
    "| 計算速度 | 速い | 遅い（大規模データでは注意） |\n",
    "| 決定境界 | 明示的でない | 明示的 |\n",
    "| 高次元データ | 得意 | やや苦手 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVMモデルの作成\n",
    "ocsvm = OneClassSVM(\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    nu=0.05  # 異常データの割合を5%と仮定\n",
    ")\n",
    "\n",
    "# 学習と予測\n",
    "# 予測結果: 1 = 正常, -1 = 異常\n",
    "ocsvm_predictions = ocsvm.fit_predict(X_scaled)\n",
    "\n",
    "# 異常スコアを計算（決定関数の値、正なら正常、負なら異常）\n",
    "ocsvm_scores = ocsvm.decision_function(X_scaled)\n",
    "\n",
    "# 結果を元のDataFrameに追加\n",
    "df['ocsvm_prediction'] = ocsvm_predictions\n",
    "df['ocsvm_score'] = ocsvm_scores\n",
    "\n",
    "print(f\"異常と判定されたデータ数: {(ocsvm_predictions == -1).sum()}件\")\n",
    "print(f\"正常と判定されたデータ数: {(ocsvm_predictions == 1).sum()}件\")\n",
    "print(f\"異常データの割合: {(ocsvm_predictions == -1).sum() / len(ocsvm_predictions) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Class SVMの結果を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異常スコアの分布を可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ヒストグラム\n",
    "axes[0].hist(df['ocsvm_score'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0].axvline(0, color='red', linestyle='--', label='決定境界（0）')\n",
    "axes[0].set_xlabel('Decision Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of One-Class SVM Decision Scores')\n",
    "axes[0].legend()\n",
    "\n",
    "# ボックスプロット（正常 vs 異常）\n",
    "df_plot = df.copy()\n",
    "df_plot['Label'] = df_plot['ocsvm_prediction'].map({1: 'Normal', -1: 'Anomaly'})\n",
    "sns.boxplot(data=df_plot, x='Label', y='ocsvm_score', ax=axes[1])\n",
    "axes[1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1].set_title('Decision Score: Normal vs Anomaly')\n",
    "axes[1].set_ylabel('Decision Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAで2次元に削減して可視化\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                     c=ocsvm_predictions, \n",
    "                     cmap='coolwarm', \n",
    "                     alpha=0.6,\n",
    "                     edgecolors='black',\n",
    "                     linewidths=0.5)\n",
    "plt.colorbar(scatter, label='Prediction (1=Normal, -1=Anomaly)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title('One-Class SVM: Normal vs Anomaly (PCA Visualization)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 異常と判定されたデータを分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異常データを抽出\n",
    "anomalies_ocsvm = df[df['ocsvm_prediction'] == -1].copy()\n",
    "\n",
    "print(\"異常と判定されたデータ（上位10件）:\")\n",
    "print(anomalies_ocsvm[['PassengerId', 'Name', 'Pclass', 'Sex', 'Age', 'Fare', \n",
    "                        'FamilySize', 'Survived', 'ocsvm_score']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 手法3: Local Outlier Factor (LOF)\n",
    "\n",
    "### Local Outlier Factorとは？\n",
    "\n",
    "**LOF**は、各データポイントの**局所密度**を計算し、周囲と比較して密度が低いデータを異常と判定する手法です。\n",
    "\n",
    "#### 基本的な考え方\n",
    "\n",
    "1. 各データポイントについて、k近傍のデータを見つける\n",
    "2. その近傍の密度を計算\n",
    "3. 近傍の密度と比較して、自分の密度が低ければ異常\n",
    "\n",
    "#### 主なパラメータ\n",
    "\n",
    "- **n_neighbors**: 近傍の数（デフォルト: 20）\n",
    "- **contamination**: 異常データの割合の推定値\n",
    "\n",
    "#### 他の手法との違い\n",
    "\n",
    "- **Isolation Forest**: グローバルな異常検知\n",
    "- **One-Class SVM**: 正常領域の境界を学習\n",
    "- **LOF**: **局所的な密度**に基づく異常検知\n",
    "  - クラスタが複数ある場合に有効\n",
    "  - 密度が異なる領域でも検出可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOFモデルの作成\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=0.05,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 学習と予測\n",
    "# 予測結果: 1 = 正常, -1 = 異常\n",
    "lof_predictions = lof.fit_predict(X_scaled)\n",
    "\n",
    "# 異常スコア（負の外れ値因子）\n",
    "# 値が小さい（より負）ほど異常\n",
    "lof_scores = lof.negative_outlier_factor_\n",
    "\n",
    "# 結果を元のDataFrameに追加\n",
    "df['lof_prediction'] = lof_predictions\n",
    "df['lof_score'] = lof_scores\n",
    "\n",
    "print(f\"異常と判定されたデータ数: {(lof_predictions == -1).sum()}件\")\n",
    "print(f\"正常と判定されたデータ数: {(lof_predictions == 1).sum()}件\")\n",
    "print(f\"異常データの割合: {(lof_predictions == -1).sum() / len(lof_predictions) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOFの結果を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異常スコアの分布を可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ヒストグラム\n",
    "axes[0].hist(df['lof_score'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0].axvline(df[df['lof_prediction'] == -1]['lof_score'].max(), \n",
    "                color='red', linestyle='--', label='異常の閾値')\n",
    "axes[0].set_xlabel('LOF Score (Negative Outlier Factor)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of LOF Scores')\n",
    "axes[0].legend()\n",
    "\n",
    "# ボックスプロット（正常 vs 異常）\n",
    "df_plot = df.copy()\n",
    "df_plot['Label'] = df_plot['lof_prediction'].map({1: 'Normal', -1: 'Anomaly'})\n",
    "sns.boxplot(data=df_plot, x='Label', y='lof_score', ax=axes[1])\n",
    "axes[1].set_title('LOF Score: Normal vs Anomaly')\n",
    "axes[1].set_ylabel('LOF Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAで2次元に削減して可視化\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                     c=lof_predictions, \n",
    "                     cmap='coolwarm', \n",
    "                     alpha=0.6,\n",
    "                     edgecolors='black',\n",
    "                     linewidths=0.5)\n",
    "plt.colorbar(scatter, label='Prediction (1=Normal, -1=Anomaly)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title('LOF: Normal vs Anomaly (PCA Visualization)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 手法の比較\n",
    "\n",
    "3つの異常検知手法（Isolation Forest、One-Class SVM、LOF）の結果を比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各手法で検出された異常データの重複を確認\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# 異常と判定されたインデックスを取得\n",
    "iso_anomalies = set(df[df['iso_prediction'] == -1].index)\n",
    "ocsvm_anomalies = set(df[df['ocsvm_prediction'] == -1].index)\n",
    "lof_anomalies = set(df[df['lof_prediction'] == -1].index)\n",
    "\n",
    "# ベン図で可視化\n",
    "plt.figure(figsize=(10, 7))\n",
    "venn3([iso_anomalies, ocsvm_anomalies, lof_anomalies], \n",
    "      set_labels=('Isolation Forest', 'One-Class SVM', 'LOF'))\n",
    "plt.title('Overlap of Anomalies Detected by Different Methods')\n",
    "plt.show()\n",
    "\n",
    "# 統計情報を表示\n",
    "print(\"各手法で検出された異常データ数:\")\n",
    "print(f\"Isolation Forest: {len(iso_anomalies)}件\")\n",
    "print(f\"One-Class SVM: {len(ocsvm_anomalies)}件\")\n",
    "print(f\"LOF: {len(lof_anomalies)}件\")\n",
    "print(f\"\\n全ての手法で異常と判定: {len(iso_anomalies & ocsvm_anomalies & lof_anomalies)}件\")\n",
    "print(f\"少なくとも1つの手法で異常と判定: {len(iso_anomalies | ocsvm_anomalies | lof_anomalies)}件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3つの手法すべてで異常と判定されたデータを確認\n",
    "all_methods_anomalies = iso_anomalies & ocsvm_anomalies & lof_anomalies\n",
    "\n",
    "if len(all_methods_anomalies) > 0:\n",
    "    print(\"全ての手法で異常と判定されたデータ:\")\n",
    "    print(df.loc[list(all_methods_anomalies), \n",
    "                 ['PassengerId', 'Name', 'Pclass', 'Sex', 'Age', 'Fare', \n",
    "                  'FamilySize', 'Survived']])\n",
    "else:\n",
    "    print(\"全ての手法で異常と判定されたデータはありません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各手法の異常スコアの相関を確認\n",
    "score_correlation = df[['iso_score', 'ocsvm_score', 'lof_score']].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(score_correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation of Anomaly Scores')\n",
    "plt.show()\n",
    "\n",
    "print(\"異常スコアの相関:\")\n",
    "print(score_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手法の比較まとめ\n",
    "\n",
    "| 手法 | メリット | デメリット | 適用場面 |\n",
    "|------|---------|-----------|----------|\n",
    "| **Isolation Forest** | ・高速<br>・高次元データに強い<br>・パラメータ調整が簡単 | ・決定境界が不明確<br>・局所的なパターンを見逃す可能性 | ・大規模データ<br>・高次元データ<br>・グローバルな異常検知 |\n",
    "| **One-Class SVM** | ・決定境界が明確<br>・非線形境界を学習可能<br>・理論的基盤が強固 | ・計算コストが高い<br>・パラメータ調整が難しい<br>・大規模データには不向き | ・小〜中規模データ<br>・明確な境界が欲しい場合<br>・正常領域が明確 |\n",
    "| **LOF** | ・局所的なパターンを検出<br>・密度が異なる領域でも検出可能<br>・クラスタごとの異常検知 | ・計算コストが高い<br>・近傍数の選択が重要<br>・大規模データには不向き | ・複数クラスタがある<br>・密度が不均一<br>・局所的な異常検知 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 応用: 生存予測への活用\n",
    "\n",
    "異常検知の結果を使って、以下の2つのアプローチを試します：\n",
    "\n",
    "1. **異常値を除外**してモデルを学習\n",
    "2. **異常スコアを特徴量として追加**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 生存予測用のデータを準備\n",
    "y = df['Survived'].values\n",
    "\n",
    "# ベースラインモデル（全データを使用）\n",
    "rf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "baseline_scores = cross_val_score(rf_baseline, X_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"ベースラインモデル（全データ）:\")\n",
    "print(f\"平均精度: {baseline_scores.mean():.4f} (+/- {baseline_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アプローチ1: 異常値を除外してモデルを学習\n",
    "# Isolation Forestで異常と判定されたデータを除外\n",
    "normal_mask = df['iso_prediction'] == 1\n",
    "X_normal = X_scaled[normal_mask]\n",
    "y_normal = y[normal_mask]\n",
    "\n",
    "rf_filtered = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "filtered_scores = cross_val_score(rf_filtered, X_normal, y_normal, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"\\n異常値除外モデル:\")\n",
    "print(f\"平均精度: {filtered_scores.mean():.4f} (+/- {filtered_scores.std():.4f})\")\n",
    "print(f\"訓練データ数: {len(X_normal)}件（{len(X_scaled) - len(X_normal)}件を除外）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アプローチ2: 異常スコアを特徴量として追加\n",
    "X_with_scores = np.column_stack([\n",
    "    X_scaled,\n",
    "    df['iso_score'].values.reshape(-1, 1),\n",
    "    df['ocsvm_score'].values.reshape(-1, 1),\n",
    "    df['lof_score'].values.reshape(-1, 1)\n",
    "])\n",
    "\n",
    "rf_with_scores = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "scores_added = cross_val_score(rf_with_scores, X_with_scores, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"\\n異常スコア追加モデル:\")\n",
    "print(f\"平均精度: {scores_added.mean():.4f} (+/- {scores_added.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を比較\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Filtered (No Anomalies)', 'With Anomaly Scores'],\n",
    "    'Mean Accuracy': [baseline_scores.mean(), filtered_scores.mean(), scores_added.mean()],\n",
    "    'Std Dev': [baseline_scores.std(), filtered_scores.std(), scores_added.std()]\n",
    "})\n",
    "\n",
    "print(\"\\n結果の比較:\")\n",
    "print(results.round(4))\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = np.arange(len(results))\n",
    "plt.bar(x_pos, results['Mean Accuracy'], yerr=results['Std Dev'], \n",
    "        alpha=0.7, capsize=10, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Models with Different Anomaly Handling')\n",
    "plt.xticks(x_pos, results['Model'], rotation=15, ha='right')\n",
    "plt.ylim(0.7, 0.85)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. まとめ\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "#### 1. 異常検知の基礎\n",
    "- 異常検知は正常データから外れたパターンを見つける教師なし学習\n",
    "- 不正検知、システム監視、品質管理など幅広い応用がある\n",
    "\n",
    "#### 2. 3つの主要な手法\n",
    "\n",
    "**Isolation Forest**\n",
    "- ランダム分割で異常を隔離\n",
    "- 高速で高次元データに強い\n",
    "- まず最初に試すべき手法\n",
    "\n",
    "**One-Class SVM**\n",
    "- 正常領域の境界を学習\n",
    "- 決定境界が明確\n",
    "- 小〜中規模データに適している\n",
    "\n",
    "**Local Outlier Factor (LOF)**\n",
    "- 局所密度に基づく検出\n",
    "- 複数クラスタや密度が不均一な場合に有効\n",
    "- 局所的な異常パターンを検出\n",
    "\n",
    "#### 3. 実践的なポイント\n",
    "\n",
    "- **複数の手法を試す**: 1つの手法だけでなく、複数を比較することが重要\n",
    "- **ドメイン知識を活用**: 検出された異常が本当に異常か、人間の目で確認\n",
    "- **contamination（異常の割合）の設定**: データに応じて適切に設定\n",
    "- **可視化が重要**: PCAなどで次元削減して結果を確認\n",
    "\n",
    "#### 4. 予測タスクへの応用\n",
    "\n",
    "- 異常値を除外することで、場合によってはモデルの精度が向上\n",
    "- 異常スコアを特徴量として追加する方法も有効\n",
    "- ただし、除外しすぎると過学習のリスクもあるので注意\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "1. **他の異常検知手法を学ぶ**\n",
    "   - Autoencoder（ディープラーニング）\n",
    "   - DBSCAN（クラスタリングベース）\n",
    "   - Elliptic Envelope（統計的手法）\n",
    "\n",
    "2. **他のデータセットで実践**\n",
    "   - クレジットカード不正検知データ\n",
    "   - ネットワーク侵入検知データ\n",
    "   - センサーデータの異常検知\n",
    "\n",
    "3. **パラメータチューニング**\n",
    "   - Grid Searchで最適なパラメータを探索\n",
    "   - 異常の割合（contamination）の影響を調査\n",
    "\n",
    "4. **評価指標を学ぶ**\n",
    "   - 正解ラベルがある場合の評価（Precision, Recall, F1-score）\n",
    "   - 正解ラベルがない場合の評価（Silhouette Score、可視化）\n",
    "\n",
    "### 参考資料\n",
    "\n",
    "- [scikit-learn: Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "- [Isolation Forest論文](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)\n",
    "- [LOF論文](https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
