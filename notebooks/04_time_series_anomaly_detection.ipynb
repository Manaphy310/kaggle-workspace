{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 時系列データの異常検知\n",
    "\n",
    "このノートブックでは、時系列データの異常検知手法を学びます。\n",
    "\n",
    "## 時系列異常検知とは？\n",
    "\n",
    "**時系列異常検知**は、時間的な順序を持つデータから、正常なパターンから外れた異常を検出する手法です。\n",
    "\n",
    "### なぜ時系列データに特別な手法が必要なのか？\n",
    "\n",
    "1. **時間的な依存関係**: 過去の値が現在の値に影響する\n",
    "2. **トレンド**: 長期的な増加・減少傾向\n",
    "3. **季節性**: 周期的なパターン（日次、週次、月次など）\n",
    "4. **突発的な変化**: スパイク、ドロップなど\n",
    "\n",
    "### 静的データ vs 時系列データ\n",
    "\n",
    "| 特徴 | 静的データ | 時系列データ |\n",
    "|------|-----------|-------------|\n",
    "| データ形式 | テーブル（行×列） | 時刻 + 値 |\n",
    "| 順序 | 関係なし | **重要** |\n",
    "| 依存関係 | 独立 | **時間的依存** |\n",
    "| 異常検知 | Isolation Forest、LOF | 統計的手法、深層学習 |\n",
    "\n",
    "### サイバー攻撃検知での応用\n",
    "\n",
    "- **DDoS攻撃**: トラフィックの急激な増加\n",
    "- **ポートスキャン**: 短時間での連続アクセス\n",
    "- **データ漏洩**: 通常と異なるデータ転送量\n",
    "- **異常ログイン**: 時間帯・頻度の異常"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ処理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 機械学習（異常検知）\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 統計\n",
    "from scipy import stats\n",
    "\n",
    "# 設定\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"ライブラリのインポート完了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 時系列異常検知の手法\n",
    "\n",
    "### 主要な手法\n",
    "\n",
    "#### 1. **統計的手法**\n",
    "- **移動平均 (Moving Average)**: 過去N期間の平均からの乖離\n",
    "- **標準偏差 (Standard Deviation)**: 平均 ± 3σ から外れたら異常\n",
    "- **Z-Score**: 標準化された値が閾値を超えたら異常\n",
    "\n",
    "#### 2. **ウィンドウベース手法**\n",
    "- 時系列をウィンドウ（スライディングウィンドウ）に分割\n",
    "- 各ウィンドウの特徴量を抽出\n",
    "- Isolation Forestなどで異常検知\n",
    "\n",
    "#### 3. **深層学習手法**（参考）\n",
    "- **LSTM Autoencoder**: 正常パターンを学習し、再構成誤差で異常判定\n",
    "- **GRU Autoencoder**: LSTMの軽量版\n",
    "- **Transformer**: 最新のアーキテクチャ\n",
    "\n",
    "このノートブックでは、**1と2の手法**を実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データの生成\n",
    "\n",
    "まず、シンプルな正弦波データで異常検知を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正弦波データの生成\n",
    "def generate_sine_wave(n_samples=1000, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    正弦波データを生成\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: サンプル数\n",
    "    - noise_level: ノイズの強度\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 50, n_samples)\n",
    "    y = np.sin(x) + np.random.normal(0, noise_level, n_samples)\n",
    "    return x, y\n",
    "\n",
    "# 異常データの生成（スパイクを追加）\n",
    "def add_anomalies(data, n_anomalies=10, spike_magnitude=3):\n",
    "    \"\"\"\n",
    "    データに異常（スパイク）を追加\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 元のデータ\n",
    "    - n_anomalies: 異常の数\n",
    "    - spike_magnitude: スパイクの大きさ\n",
    "    \"\"\"\n",
    "    data_with_anomalies = data.copy()\n",
    "    anomaly_indices = np.random.choice(len(data), n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        data_with_anomalies[idx] += spike_magnitude * np.random.choice([-1, 1])\n",
    "    \n",
    "    return data_with_anomalies, anomaly_indices\n",
    "\n",
    "# データ生成\n",
    "time, normal_data = generate_sine_wave(n_samples=1000, noise_level=0.1)\n",
    "test_data, anomaly_indices = add_anomalies(normal_data, n_anomalies=15, spike_magnitude=3)\n",
    "\n",
    "print(f\"正常データのサイズ: {normal_data.shape}\")\n",
    "print(f\"テストデータのサイズ: {test_data.shape}\")\n",
    "print(f\"異常の数: {len(anomaly_indices)}\")\n",
    "print(f\"異常の位置（最初の10個）: {sorted(anomaly_indices)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの可視化\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# 正常データ\n",
    "axes[0].plot(time, normal_data, label='Normal Data', color='blue', alpha=0.7)\n",
    "axes[0].set_title('Normal Sine Wave', fontsize=14)\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 異常データ\n",
    "axes[1].plot(time, test_data, label='Test Data', color='blue', alpha=0.7)\n",
    "axes[1].scatter(time[anomaly_indices], test_data[anomaly_indices], \n",
    "                color='red', s=100, label='True Anomalies', zorder=5)\n",
    "axes[1].set_title('Sine Wave with Anomalies (Spikes)', fontsize=14)\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 手法1: 統計的異常検知\n",
    "\n",
    "### 4.1 移動平均と標準偏差による検知\n",
    "\n",
    "**仕組み:**\n",
    "1. 移動平均（MA）を計算: 過去N期間の平均\n",
    "2. 移動標準偏差（MSD）を計算: 過去N期間の標準偏差\n",
    "3. 閾値を設定: `MA ± k × MSD` (通常 k=3)\n",
    "4. 閾値を超えたら異常と判定\n",
    "\n",
    "**メリット:**\n",
    "- シンプルで理解しやすい\n",
    "- 計算が高速\n",
    "- リアルタイム処理に適している\n",
    "\n",
    "**デメリット:**\n",
    "- 複雑なパターンは検出困難\n",
    "- ウィンドウサイズの選択が重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_statistical(data, window_size=20, k=3):\n",
    "    \"\"\"\n",
    "    移動平均と標準偏差を使った異常検知\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 時系列データ\n",
    "    - window_size: 移動平均のウィンドウサイズ\n",
    "    - k: 標準偏差の倍数（通常3）\n",
    "    \n",
    "    Returns:\n",
    "    - anomalies: 異常フラグ (True/False)\n",
    "    - ma: 移動平均\n",
    "    - upper_bound: 上限閾値\n",
    "    - lower_bound: 下限閾値\n",
    "    \"\"\"\n",
    "    # DataFrameに変換\n",
    "    df = pd.DataFrame({'value': data})\n",
    "    \n",
    "    # 移動平均と移動標準偏差\n",
    "    df['ma'] = df['value'].rolling(window=window_size, center=False).mean()\n",
    "    df['mstd'] = df['value'].rolling(window=window_size, center=False).std()\n",
    "    \n",
    "    # 閾値の計算\n",
    "    df['upper_bound'] = df['ma'] + k * df['mstd']\n",
    "    df['lower_bound'] = df['ma'] - k * df['mstd']\n",
    "    \n",
    "    # 異常判定\n",
    "    df['anomaly'] = (df['value'] > df['upper_bound']) | (df['value'] < df['lower_bound'])\n",
    "    \n",
    "    return df['anomaly'].values, df['ma'].values, df['upper_bound'].values, df['lower_bound'].values\n",
    "\n",
    "# 異常検知の実行\n",
    "WINDOW_SIZE = 30\n",
    "K_SIGMA = 3\n",
    "\n",
    "stat_anomalies, ma, upper_bound, lower_bound = detect_anomalies_statistical(\n",
    "    test_data, \n",
    "    window_size=WINDOW_SIZE, \n",
    "    k=K_SIGMA\n",
    ")\n",
    "\n",
    "print(f\"検出された異常数: {stat_anomalies.sum()}\")\n",
    "print(f\"異常の割合: {stat_anomalies.sum() / len(stat_anomalies) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の可視化\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# 元のデータ\n",
    "plt.plot(time, test_data, label='Data', color='blue', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "# 移動平均\n",
    "plt.plot(time, ma, label='Moving Average', color='green', linewidth=2)\n",
    "\n",
    "# 閾値\n",
    "plt.plot(time, upper_bound, label='Upper Bound (MA + 3σ)', color='orange', \n",
    "         linestyle='--', linewidth=1.5)\n",
    "plt.plot(time, lower_bound, label='Lower Bound (MA - 3σ)', color='orange', \n",
    "         linestyle='--', linewidth=1.5)\n",
    "\n",
    "# 真の異常\n",
    "plt.scatter(time[anomaly_indices], test_data[anomaly_indices], \n",
    "            color='red', s=150, label='True Anomalies', zorder=5, marker='o', \n",
    "            edgecolors='black', linewidths=2)\n",
    "\n",
    "# 検出された異常\n",
    "detected_indices = np.where(stat_anomalies)[0]\n",
    "plt.scatter(time[detected_indices], test_data[detected_indices], \n",
    "            color='yellow', s=80, label='Detected Anomalies', zorder=4, \n",
    "            marker='x', linewidths=2)\n",
    "\n",
    "plt.title('Statistical Anomaly Detection (Moving Average + 3σ)', fontsize=14)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価\n",
    "# 真のラベルを作成（異常=True, 正常=False）\n",
    "true_labels = np.zeros(len(test_data), dtype=bool)\n",
    "true_labels[anomaly_indices] = True\n",
    "\n",
    "# NaNを正常として扱う（移動平均の初期値）\n",
    "stat_anomalies_clean = np.nan_to_num(stat_anomalies, nan=False)\n",
    "\n",
    "# 評価指標\n",
    "precision_stat = precision_score(true_labels, stat_anomalies_clean, zero_division=0)\n",
    "recall_stat = recall_score(true_labels, stat_anomalies_clean, zero_division=0)\n",
    "f1_stat = f1_score(true_labels, stat_anomalies_clean, zero_division=0)\n",
    "\n",
    "print(\"=== 統計的手法の評価 ===\")\n",
    "print(f\"Precision: {precision_stat:.4f}\")\n",
    "print(f\"Recall: {recall_stat:.4f}\")\n",
    "print(f\"F1-Score: {f1_stat:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, stat_anomalies_clean, \n",
    "                            target_names=['Normal', 'Anomaly'], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Z-Scoreによる異常検知\n",
    "\n",
    "**Z-Score**は、データが平均からどれだけ離れているかを標準偏差の単位で表します。\n",
    "\n",
    "```\n",
    "Z = (x - μ) / σ\n",
    "```\n",
    "\n",
    "- **|Z| > 3**: 異常と判定（99.7%信頼区間外）\n",
    "- **|Z| > 2**: やや異常（95%信頼区間外）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Scoreの計算\n",
    "z_scores = np.abs(stats.zscore(test_data))\n",
    "\n",
    "# 閾値を設定（|Z| > 3）\n",
    "threshold_z = 3\n",
    "zscore_anomalies = z_scores > threshold_z\n",
    "\n",
    "print(f\"検出された異常数: {zscore_anomalies.sum()}\")\n",
    "print(f\"異常の割合: {zscore_anomalies.sum() / len(zscore_anomalies) * 100:.2f}%\")\n",
    "\n",
    "# 評価\n",
    "precision_z = precision_score(true_labels, zscore_anomalies, zero_division=0)\n",
    "recall_z = recall_score(true_labels, zscore_anomalies, zero_division=0)\n",
    "f1_z = f1_score(true_labels, zscore_anomalies, zero_division=0)\n",
    "\n",
    "print(\"\\n=== Z-Score手法の評価 ===\")\n",
    "print(f\"Precision: {precision_z:.4f}\")\n",
    "print(f\"Recall: {recall_z:.4f}\")\n",
    "print(f\"F1-Score: {f1_z:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Scoreの可視化\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# 元のデータ\n",
    "axes[0].plot(time, test_data, label='Data', color='blue', alpha=0.7)\n",
    "axes[0].scatter(time[anomaly_indices], test_data[anomaly_indices], \n",
    "                color='red', s=150, label='True Anomalies', zorder=5)\n",
    "detected_z = np.where(zscore_anomalies)[0]\n",
    "axes[0].scatter(time[detected_z], test_data[detected_z], \n",
    "                color='yellow', s=80, label='Detected (Z-Score)', zorder=4, marker='x')\n",
    "axes[0].set_title('Z-Score Anomaly Detection', fontsize=14)\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Z-Score\n",
    "axes[1].plot(time, z_scores, label='Z-Score', color='purple', alpha=0.7)\n",
    "axes[1].axhline(threshold_z, color='red', linestyle='--', label=f'Threshold (|Z| = {threshold_z})')\n",
    "axes[1].scatter(time[detected_z], z_scores[detected_z], \n",
    "                color='red', s=100, label='Detected Anomalies', zorder=5)\n",
    "axes[1].set_title('Z-Score over Time', fontsize=14)\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('|Z-Score|')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 手法2: ウィンドウベース + Isolation Forest\n",
    "\n",
    "時系列データをウィンドウ（スライディングウィンドウ）に分割し、各ウィンドウから特徴量を抽出してIsolation Forestで異常検知します。\n",
    "\n",
    "### ウィンドウとは？\n",
    "\n",
    "連続したデータポイントの塊です。\n",
    "\n",
    "```\n",
    "元データ: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "window_size=3\n",
    "\n",
    "ウィンドウ1: [1, 2, 3]\n",
    "ウィンドウ2: [2, 3, 4]\n",
    "ウィンドウ3: [3, 4, 5]\n",
    "...\n",
    "```\n",
    "\n",
    "### 特徴量の抽出\n",
    "\n",
    "各ウィンドウから統計的特徴量を計算:\n",
    "- 平均 (mean)\n",
    "- 標準偏差 (std)\n",
    "- 最小値 (min)\n",
    "- 最大値 (max)\n",
    "- 範囲 (range = max - min)\n",
    "- 勾配 (slope): 線形回帰の傾き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_features(data, window_size=30):\n",
    "    \"\"\"\n",
    "    時系列データからウィンドウベースの特徴量を抽出\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 1次元時系列データ\n",
    "    - window_size: ウィンドウサイズ\n",
    "    \n",
    "    Returns:\n",
    "    - features: (サンプル数, 特徴量数)の2次元配列\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        window = data[i:i + window_size]\n",
    "        \n",
    "        # 統計的特徴量\n",
    "        mean = np.mean(window)\n",
    "        std = np.std(window)\n",
    "        min_val = np.min(window)\n",
    "        max_val = np.max(window)\n",
    "        range_val = max_val - min_val\n",
    "        \n",
    "        # 勾配（線形回帰の傾き）\n",
    "        x = np.arange(window_size)\n",
    "        slope = np.polyfit(x, window, 1)[0]\n",
    "        \n",
    "        features.append([mean, std, min_val, max_val, range_val, slope])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# ウィンドウサイズの設定\n",
    "WINDOW_SIZE = 30\n",
    "\n",
    "# 正常データから特徴量を抽出（訓練用）\n",
    "X_train_features = create_windowed_features(normal_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "# テストデータから特徴量を抽出\n",
    "X_test_features = create_windowed_features(test_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "print(f\"訓練データの形状: {X_train_features.shape}\")\n",
    "print(f\"テストデータの形状: {X_test_features.shape}\")\n",
    "print(f\"\\n特徴量: [mean, std, min, max, range, slope]\")\n",
    "print(f\"\\n最初のウィンドウの特徴量:\")\n",
    "print(X_test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量の標準化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "\n",
    "print(f\"標準化後の訓練データの形状: {X_train_scaled.shape}\")\n",
    "print(f\"標準化後のテストデータの形状: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forestによる異常検知\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.05,  # 異常の割合を5%と仮定\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 訓練データで学習\n",
    "iso_forest.fit(X_train_scaled)\n",
    "\n",
    "# テストデータで予測（-1: 異常, 1: 正常）\n",
    "iso_predictions = iso_forest.predict(X_test_scaled)\n",
    "iso_anomalies_window = iso_predictions == -1\n",
    "\n",
    "# 異常スコア\n",
    "iso_scores = iso_forest.score_samples(X_test_scaled)\n",
    "\n",
    "print(f\"検出された異常ウィンドウ数: {iso_anomalies_window.sum()}\")\n",
    "print(f\"異常の割合: {iso_anomalies_window.sum() / len(iso_anomalies_window) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ウィンドウレベルの異常を元の時系列に変換\n",
    "# ウィンドウが異常なら、そのウィンドウに含まれる全ての時点を異常としてマーク\n",
    "iso_anomalies_timeseries = np.zeros(len(test_data), dtype=bool)\n",
    "\n",
    "for i, is_anomaly in enumerate(iso_anomalies_window):\n",
    "    if is_anomaly:\n",
    "        # ウィンドウ全体を異常としてマーク\n",
    "        start = i\n",
    "        end = i + WINDOW_SIZE\n",
    "        iso_anomalies_timeseries[start:end] = True\n",
    "\n",
    "print(f\"異常とマークされた時点の数: {iso_anomalies_timeseries.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の可視化\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# 元のデータと異常検知結果\n",
    "axes[0].plot(time, test_data, label='Data', color='blue', alpha=0.7, linewidth=1.5)\n",
    "axes[0].scatter(time[anomaly_indices], test_data[anomaly_indices], \n",
    "                color='red', s=150, label='True Anomalies', zorder=5, \n",
    "                edgecolors='black', linewidths=2)\n",
    "\n",
    "# 検出された異常領域を塗りつぶし\n",
    "for i, is_anomaly in enumerate(iso_anomalies_window):\n",
    "    if is_anomaly:\n",
    "        start = i\n",
    "        end = i + WINDOW_SIZE\n",
    "        axes[0].axvspan(time[start], time[min(end, len(time)-1)], \n",
    "                       alpha=0.3, color='yellow')\n",
    "\n",
    "axes[0].set_title('Window-based Isolation Forest Anomaly Detection', fontsize=14)\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 異常スコア\n",
    "# ウィンドウの中心時刻を計算\n",
    "window_centers = time[:len(iso_scores)] + WINDOW_SIZE // 2\n",
    "axes[1].plot(window_centers, iso_scores, label='Anomaly Score', color='green', alpha=0.7)\n",
    "axes[1].scatter(window_centers[iso_anomalies_window], iso_scores[iso_anomalies_window],\n",
    "                color='red', s=100, label='Detected Anomalies', zorder=5)\n",
    "axes[1].set_title('Isolation Forest Anomaly Scores', fontsize=14)\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Anomaly Score (lower = more anomalous)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価\n",
    "precision_iso = precision_score(true_labels, iso_anomalies_timeseries, zero_division=0)\n",
    "recall_iso = recall_score(true_labels, iso_anomalies_timeseries, zero_division=0)\n",
    "f1_iso = f1_score(true_labels, iso_anomalies_timeseries, zero_division=0)\n",
    "\n",
    "print(\"=== ウィンドウベースIsolation Forestの評価 ===\")\n",
    "print(f\"Precision: {precision_iso:.4f}\")\n",
    "print(f\"Recall: {recall_iso:.4f}\")\n",
    "print(f\"F1-Score: {f1_iso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 手法の比較\n",
    "\n",
    "3つの手法を比較してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の比較\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Statistical (MA + 3σ)', 'Z-Score', 'Window-based Isolation Forest'],\n",
    "    'Precision': [precision_stat, precision_z, precision_iso],\n",
    "    'Recall': [recall_stat, recall_z, recall_iso],\n",
    "    'F1-Score': [f1_stat, f1_z, f1_iso]\n",
    "})\n",
    "\n",
    "print(\"\\n=== 手法の比較 ===\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, comparison_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, comparison_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, comparison_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Method', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Comparison of Time Series Anomaly Detection Methods', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Method'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3つの手法の検出結果を一つの図で比較\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 16))\n",
    "\n",
    "# 真の異常\n",
    "axes[0].plot(time, test_data, label='Data', color='blue', alpha=0.7)\n",
    "axes[0].scatter(time[anomaly_indices], test_data[anomaly_indices], \n",
    "                color='red', s=150, label='True Anomalies', zorder=5)\n",
    "axes[0].set_title('Ground Truth (True Anomalies)', fontsize=14)\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 統計的手法\n",
    "axes[1].plot(time, test_data, label='Data', color='blue', alpha=0.3)\n",
    "detected_stat = np.where(stat_anomalies_clean)[0]\n",
    "axes[1].scatter(time[detected_stat], test_data[detected_stat], \n",
    "                color='orange', s=80, label='Detected', marker='x')\n",
    "axes[1].scatter(time[anomaly_indices], test_data[anomaly_indices], \n",
    "                color='red', s=150, label='True Anomalies', zorder=5, alpha=0.5)\n",
    "axes[1].set_title(f'Statistical Method (MA + 3σ) - F1={f1_stat:.3f}', fontsize=14)\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Z-Score\n",
    "axes[2].plot(time, test_data, label='Data', color='blue', alpha=0.3)\n",
    "axes[2].scatter(time[detected_z], test_data[detected_z], \n",
    "                color='purple', s=80, label='Detected', marker='x')\n",
    "axes[2].scatter(time[anomaly_indices], test_data[anomaly_indices], \n",
    "                color='red', s=150, label='True Anomalies', zorder=5, alpha=0.5)\n",
    "axes[2].set_title(f'Z-Score Method - F1={f1_z:.3f}', fontsize=14)\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].set_ylabel('Value')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Isolation Forest\n",
    "axes[3].plot(time, test_data, label='Data', color='blue', alpha=0.3)\n",
    "for i, is_anomaly in enumerate(iso_anomalies_window):\n",
    "    if is_anomaly:\n",
    "        start = i\n",
    "        end = i + WINDOW_SIZE\n",
    "        axes[3].axvspan(time[start], time[min(end, len(time)-1)], \n",
    "                       alpha=0.3, color='green', label='Detected' if i == np.where(iso_anomalies_window)[0][0] else '')\n",
    "axes[3].scatter(time[anomaly_indices], test_data[anomaly_indices], \n",
    "                color='red', s=150, label='True Anomalies', zorder=5, alpha=0.5)\n",
    "axes[3].set_title(f'Window-based Isolation Forest - F1={f1_iso:.3f}', fontsize=14)\n",
    "axes[3].set_xlabel('Time')\n",
    "axes[3].set_ylabel('Value')\n",
    "axes[3].legend()\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. まとめ\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "#### 1. 時系列異常検知の重要性\n",
    "- 時系列データには**時間的な依存関係**がある\n",
    "- 静的データ向けの手法だけでは不十分\n",
    "- 過去のパターンを考慮する必要がある\n",
    "\n",
    "#### 2. 3つの主要手法\n",
    "\n",
    "**統計的手法（移動平均 + 標準偏差）**\n",
    "- ✅ シンプルで理解しやすい\n",
    "- ✅ 高速、リアルタイム処理に適している\n",
    "- ❌ 複雑なパターンは検出困難\n",
    "- **適用**: システム監視、簡単な異常検知\n",
    "\n",
    "**Z-Score**\n",
    "- ✅ 非常にシンプル\n",
    "- ✅ 統計的に明確な基準\n",
    "- ❌ 時間的依存関係を考慮しない\n",
    "- **適用**: 簡易的な異常検知、前処理\n",
    "\n",
    "**ウィンドウベース + Isolation Forest**\n",
    "- ✅ 時間的な文脈を考慮\n",
    "- ✅ 複雑なパターンも検出可能\n",
    "- ✅ 特徴量エンジニアリングで柔軟性が高い\n",
    "- ❌ やや複雑、計算コストがやや高い\n",
    "- **適用**: DDoS攻撃検知、複雑な異常パターン\n",
    "\n",
    "### 手法の選択基準\n",
    "\n",
    "| 状況 | 推奨手法 |\n",
    "|------|----------|\n",
    "| リアルタイム処理が必須 | 統計的手法 |\n",
    "| シンプルな異常（スパイクなど） | Z-Score、統計的手法 |\n",
    "| 複雑なパターン | ウィンドウベース + Isolation Forest |\n",
    "| 時間的依存関係が重要 | ウィンドウベース手法 |\n",
    "| 非常に複雑なパターン | LSTM Autoencoder（深層学習） |\n",
    "\n",
    "### DDoS攻撃検知への応用\n",
    "\n",
    "#### 実務での活用例\n",
    "\n",
    "```python\n",
    "# トラフィックデータの時系列\n",
    "時系列特徴量:\n",
    "- リクエスト数/秒\n",
    "- パケットサイズの平均\n",
    "- 送信元IPの多様性\n",
    "- レスポンス時間\n",
    "\n",
    "# 多層検知システム\n",
    "1. リアルタイム層（統計的手法）\n",
    "   - 移動平均 + 3σで即座にアラート\n",
    "   - 計算が高速、遅延なし\n",
    "\n",
    "2. 詳細分析層（ウィンドウベース）\n",
    "   - Isolation Forestで複雑なパターンを検出\n",
    "   - 誤検知を減らす\n",
    "\n",
    "3. 高度分析層（深層学習）\n",
    "   - LSTM Autoencoderで新種の攻撃を検出\n",
    "   - オフラインで詳細分析\n",
    "```\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "#### 1. 実際のサイバー攻撃データセットで実践\n",
    "- **KDD Cup 99**: 古典的なネットワーク侵入検知データ\n",
    "- **NSL-KDD**: KDD Cup 99の改良版\n",
    "- **CIC-IDS2017/2018**: 最新のサイバー攻撃データ\n",
    "- **UNSW-NB15**: 現代的なネットワークトラフィックデータ\n",
    "\n",
    "#### 2. 深層学習手法の学習\n",
    "- **LSTM Autoencoder**: 時系列の正常パターンを学習\n",
    "- **GRU Autoencoder**: LSTMの軽量版\n",
    "- **Transformer**: 最新のアーキテクチャ\n",
    "- **VAE (Variational Autoencoder)**: 確率的なAutoencoder\n",
    "\n",
    "#### 3. 多変量時系列への拡張\n",
    "- 複数の特徴量を同時に扱う\n",
    "- 特徴量間の相関を考慮\n",
    "- より複雑な異常パターンを検出\n",
    "\n",
    "#### 4. リアルタイムシステムの構築\n",
    "- ストリーミングデータへの対応\n",
    "- オンライン学習\n",
    "- アラートシステムとの統合\n",
    "\n",
    "### 参考資料\n",
    "\n",
    "- [scikit-learn: Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "- [Time Series Anomaly Detection](https://en.wikipedia.org/wiki/Anomaly_detection)\n",
    "- [LSTM for Time Series](https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/)\n",
    "- [DDoS Detection using Machine Learning](https://arxiv.org/abs/1804.00932)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
