Kaggleã‚³ãƒ³ãƒšã®å…·ä½“çš„ãªæµã‚Œ
Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«å‚åŠ ã™ã‚‹éš›ã®å…¨ä½“ã®æµã‚Œã‚’ã€åˆå¿ƒè€…å‘ã‘ã«ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§èª¬æ˜ã—ã¾ã™ã€‚
ğŸ“‹ ã‚³ãƒ³ãƒšå‚åŠ ã®å…¨ä½“ãƒ•ãƒ­ãƒ¼
Phase 1: ã‚³ãƒ³ãƒšé¸æŠãƒ»å‚åŠ ç™»éŒ²
Phase 2: ãƒ‡ãƒ¼ã‚¿ç†è§£ãƒ»æ¢ç´¢
Phase 3: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
Phase 4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
Phase 5: ãƒ¢ãƒ‡ãƒ«æ”¹å–„
Phase 6: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ»æœ€çµ‚èª¿æ•´
Phase 7: æå‡ºãƒ»ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ç¢ºèª
Phase 1: ã‚³ãƒ³ãƒšé¸æŠãƒ»å‚åŠ ç™»éŒ² ğŸ¯
1-1. ã‚³ãƒ³ãƒšã‚’æ¢ã™
Kaggle Competitions ã«ã‚¢ã‚¯ã‚»ã‚¹
åˆå¿ƒè€…å‘ã‘ãªã‚‰ã€ŒGetting Startedã€ã‚«ãƒ†ã‚´ãƒªãƒ¼ãŒãŠã™ã™ã‚
ä¾‹: Titanic, House Prices, Digit Recognizer
1-2. ã‚³ãƒ³ãƒšã«å‚åŠ 
ã‚³ãƒ³ãƒšãƒšãƒ¼ã‚¸ã§ã€ŒJoin Competitionã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
ãƒ«ãƒ¼ãƒ«ï¼ˆRulesï¼‰ã‚’ç¢ºèªãƒ»åŒæ„
1-3. é‡è¦æƒ…å ±ã®ç¢ºèª
Evaluation: è©•ä¾¡æŒ‡æ¨™ï¼ˆRMSEã€AUCã€Accuracyãªã©ï¼‰
Timeline: ç· ã‚åˆ‡ã‚Š
Prize: è³é‡‘ï¼ˆã‚ã‚Œã°ï¼‰
Data: ãƒ‡ãƒ¼ã‚¿ã®èª¬æ˜
Phase 2: ãƒ‡ãƒ¼ã‚¿ç†è§£ãƒ»æ¢ç´¢ ğŸ”
2-1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
æ–¹æ³•1: Web UIã‹ã‚‰
Kaggleã‚µã‚¤ãƒˆ â†’ Data ã‚¿ãƒ– â†’ Download All
æ–¹æ³•2: Kaggle APIï¼ˆæ¨å¥¨ï¼‰
# ã‚³ãƒ³ãƒšåã‚’ç¢ºèªï¼ˆURLã‹ã‚‰ï¼‰
# ä¾‹: https://www.kaggle.com/c/house-prices-advanced-regression-techniques
# â†’ ã‚³ãƒ³ãƒšåã¯ "house-prices-advanced-regression-techniques"

uv run kaggle competitions download -c house-prices-advanced-regression-techniques
unzip house-prices-advanced-regression-techniques.zip -d data/raw/
2-2. ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
ls data/raw/
# é€šå¸¸ã®æ§‹æˆä¾‹:
# train.csv        - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ä»˜ãï¼‰
# test.csv         - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆäºˆæ¸¬å¯¾è±¡ï¼‰
# sample_submission.csv - æå‡ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ä¾‹
# data_description.txt  - ãƒ‡ãƒ¼ã‚¿ã®èª¬æ˜
2-3. EDA (Exploratory Data Analysis)
notebooks/01_eda.ipynb ã§å®Ÿæ–½ï¼š
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
train = pd.read_csv('../data/raw/train.csv')
test = pd.read_csv('../data/raw/test.csv')

# åŸºæœ¬æƒ…å ±
print(train.shape)  # (è¡Œæ•°, åˆ—æ•°)
train.head()
train.info()        # ãƒ‡ãƒ¼ã‚¿å‹ã€æ¬ æå€¤
train.describe()    # çµ±è¨ˆé‡

# ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒ
plt.hist(train['target'])
plt.title('Target Distribution')

# æ¬ æå€¤ã®ç¢ºèª
train.isnull().sum()

# ç›¸é–¢åˆ†æ
correlation = train.corr()
sns.heatmap(correlation, annot=True)

# ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®é–¢ä¿‚
sns.scatterplot(x='feature1', y='target', data=train)
ğŸ‘€ EDAã§ç¢ºèªã™ã‚‹ã“ã¨
âœ… ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆè¡Œæ•°ãƒ»åˆ—æ•°ï¼‰
âœ… æ¬ æå€¤ã®æœ‰ç„¡ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³
âœ… ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒï¼ˆåã‚Šã¯ãªã„ã‹ï¼‰
âœ… ç‰¹å¾´é‡ã®å‹ï¼ˆæ•°å€¤ãƒ»ã‚«ãƒ†ã‚´ãƒªï¼‰
âœ… å¤–ã‚Œå€¤ã®æœ‰ç„¡
âœ… ç‰¹å¾´é‡é–“ã®ç›¸é–¢
âœ… train/testã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®é•ã„
Phase 3: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ ğŸš€
3-1. ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã§æœ€åˆã®æå‡º
notebooks/02_baseline.ipynb:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
train = pd.read_csv('../data/raw/train.csv')
test = pd.read_csv('../data/raw/test.csv')

# ã‚·ãƒ³ãƒ—ãƒ«ãªå‰å‡¦ç†
# æ•°å€¤åˆ—ã®ã¿ä½¿ç”¨ã€æ¬ æå€¤ã¯ä¸­å¤®å€¤ã§åŸ‹ã‚ã‚‹
numeric_features = train.select_dtypes(include=[np.number]).columns
numeric_features = numeric_features.drop('target')  # ç›®çš„å¤‰æ•°ã‚’é™¤å¤–

X = train[numeric_features].fillna(train[numeric_features].median())
y = train['target']
X_test = test[numeric_features].fillna(train[numeric_features].median())

# è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# æ¤œè¨¼ã‚¹ã‚³ã‚¢
y_pred = model.predict(X_valid)
score = mean_squared_error(y_valid, y_pred, squared=False)  # RMSE
print(f'Validation RMSE: {score}')

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
predictions = model.predict(X_test)

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
submission = pd.DataFrame({
    'id': test['id'],
    'target': predictions
})
submission.to_csv('../submissions/baseline_submission.csv', index=False)
3-2. åˆå›æå‡º
# Web UIã‹ã‚‰ submissions/baseline_submission.csv ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# ã¾ãŸã¯
uv run kaggle competitions submit \
    -c house-prices-advanced-regression-techniques \
    -f submissions/baseline_submission.csv \
    -m "Baseline: Random Forest with numeric features only"
3-3. ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã®ç¢ºèª
Public Leaderboard: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã§è©•ä¾¡ï¼ˆã‚³ãƒ³ãƒšæœŸé–“ä¸­ã«è¦‹ãˆã‚‹ï¼‰
Private Leaderboard: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ®‹ã‚Šéƒ¨åˆ†ã§è©•ä¾¡ï¼ˆçµ‚äº†å¾Œã«å…¬é–‹ï¼‰
Phase 4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ğŸ› ï¸
4-1. ç‰¹å¾´é‡ã®ä½œæˆãƒ»æ”¹å–„
notebooks/03_feature_engineering.ipynb ã§å®Ÿé¨“ï¼š
# æ¬ æå€¤ã®æ‰±ã„ã‚’æ”¹å–„
train['feature1_is_missing'] = train['feature1'].isnull().astype(int)

# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
train['category_encoded'] = le.fit_transform(train['category'])

# æ–°ã—ã„ç‰¹å¾´é‡ã®ä½œæˆ
train['feature_interaction'] = train['feature1'] * train['feature2']
train['feature_ratio'] = train['feature1'] / (train['feature2'] + 1)

# å¯¾æ•°å¤‰æ›ï¼ˆåˆ†å¸ƒãŒåã£ã¦ã„ã‚‹å ´åˆï¼‰
train['feature_log'] = np.log1p(train['feature1'])

# æ—¥ä»˜ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º
train['year'] = pd.to_datetime(train['date']).dt.year
train['month'] = pd.to_datetime(train['date']).dt.month
train['day_of_week'] = pd.to_datetime(train['date']).dt.dayofweek

# é›†ç´„ç‰¹å¾´é‡
train['category_mean_target'] = train.groupby('category')['target'].transform('mean')
4-2. é–¢æ•°åŒ–ã—ã¦å†åˆ©ç”¨å¯èƒ½ã«
è‰¯ã„ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ src/features/feature_engineering.py ã«ç§»ã™ï¼š
def create_features(df):
    """ç‰¹å¾´é‡ã‚’ä½œæˆ"""
    df = df.copy()
    
    # æ¬ æãƒ•ãƒ©ã‚°
    df['feature1_is_missing'] = df['feature1'].isnull().astype(int)
    
    # ç›¸äº’ä½œç”¨
    df['feature_interaction'] = df['feature1'] * df['feature2']
    
    # å¯¾æ•°å¤‰æ›
    df['feature_log'] = np.log1p(df['feature1'])
    
    return df
Phase 5: ãƒ¢ãƒ‡ãƒ«æ”¹å–„ ğŸ“ˆ
5-1. ã‚ˆã‚Šå¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
# LightGBMï¼ˆå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼‰
import lightgbm as lgb

params = {
    'objective': 'regression',
    'metric': 'rmse',
    'learning_rate': 0.05,
    'num_leaves': 31,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
}

train_data = lgb.Dataset(X_train, y_train)
valid_data = lgb.Dataset(X_valid, y_valid, reference=train_data)

model = lgb.train(
    params,
    train_data,
    num_boost_round=1000,
    valid_sets=[valid_data],
    callbacks=[lgb.early_stopping(50)]
)
5-2. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = []

for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):
    X_train_fold = X.iloc[train_idx]
    y_train_fold = y.iloc[train_idx]
    X_valid_fold = X.iloc[valid_idx]
    y_valid_fold = y.iloc[valid_idx]
    
    model.fit(X_train_fold, y_train_fold)
    pred = model.predict(X_valid_fold)
    score = mean_squared_error(y_valid_fold, pred, squared=False)
    scores.append(score)
    print(f'Fold {fold+1}: {score}')

print(f'Average CV Score: {np.mean(scores)}')
5-3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# Optunaï¼ˆè‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
import optuna

def objective(trial):
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
    }
    
    model = lgb.train(params, train_data, num_boost_round=100)
    pred = model.predict(X_valid)
    return mean_squared_error(y_valid, pred, squared=False)

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)
print(f'Best params: {study.best_params}')
Phase 6: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ»æœ€çµ‚èª¿æ•´ ğŸ¯
6-1. è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å¹³å‡
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import lightgbm as lgb

# ãƒ¢ãƒ‡ãƒ«1: LightGBM
model1 = lgb.train(params1, train_data)
pred1 = model1.predict(X_test)

# ãƒ¢ãƒ‡ãƒ«2: Random Forest
model2 = RandomForestRegressor()
model2.fit(X_train, y_train)
pred2 = model2.predict(X_test)

# ãƒ¢ãƒ‡ãƒ«3: XGBoost
import xgboost as xgb
model3 = xgb.XGBRegressor()
model3.fit(X_train, y_train)
pred3 = model3.predict(X_test)

# åŠ é‡å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
final_pred = 0.5 * pred1 + 0.3 * pred2 + 0.2 * pred3
6-2. ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°
from sklearn.linear_model import Ridge

# Level 1: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
# Level 2: ãã‚Œã‚‰ã‚’å…¥åŠ›ã¨ã—ã¦æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’
meta_features = np.column_stack([pred1, pred2, pred3])
meta_model = Ridge()
meta_model.fit(meta_features_train, y_train)
final_pred = meta_model.predict(meta_features_test)
Phase 7: æå‡ºãƒ»ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ç¢ºèª ğŸ“Š
7-1. è¤‡æ•°å›æå‡º
Kaggleã§ã¯1æ—¥ã®æå‡ºå›æ•°ã«åˆ¶é™ãŒã‚ã‚Šã¾ã™ï¼ˆã‚³ãƒ³ãƒšã«ã‚ˆã‚‹ï¼‰ï¼š
ä¾‹: 1æ—¥5å›ã¾ã§
æˆ¦ç•¥ï¼š
åˆæœŸ: è‰²ã€…è©¦ã—ã¦é »ç¹ã«æå‡º
ä¸­æœŸ: CVï¼ˆã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚¹ã‚³ã‚¢ã‚’ä¿¡é ¼ã—ã¦æå‡ºã‚’çµã‚‹
çµ‚ç›¤: æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«2ã¤ã‚’é¸ã‚“ã§æå‡º
7-2. Public vs Private LB
Public LB (30%):  ã‚³ãƒ³ãƒšæœŸé–“ä¸­ã«è¦‹ãˆã‚‹ã‚¹ã‚³ã‚¢
Private LB (70%): ã‚³ãƒ³ãƒšçµ‚äº†å¾Œã«åˆ¤æ˜ã™ã‚‹æœ€çµ‚é †ä½
æ³¨æ„: Public LBã«éå­¦ç¿’ã—ãªã„ï¼
CVã‚¹ã‚³ã‚¢ã¨Public LBã‚¹ã‚³ã‚¢ã®ç›¸é–¢ã‚’ç¢ºèª
CVã‚¹ã‚³ã‚¢ã‚’ä¿¡é ¼ã™ã‚‹
7-3. æœ€çµ‚æå‡ºã®é¸æŠ
ã‚³ãƒ³ãƒšçµ‚äº†å‰ã«2ã¤ã®æå‡ºã‚’é¸æŠï¼š
æœ€ã‚‚CVã‚¹ã‚³ã‚¢ãŒè‰¯ã„ã‚‚ã®
æœ€ã‚‚Public LBã‚¹ã‚³ã‚¢ãŒè‰¯ã„ã‚‚ã®ï¼ˆç•°ãªã‚‹å ´åˆï¼‰
ğŸ—“ï¸ ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä¾‹ï¼ˆ3ãƒ¶æœˆã‚³ãƒ³ãƒšã®å ´åˆï¼‰
æœŸé–“	ãƒ•ã‚§ãƒ¼ã‚º	ã‚„ã‚‹ã“ã¨
Week 1	ãƒ‡ãƒ¼ã‚¿ç†è§£	EDAã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æå‡º
Week 2-4	ç‰¹å¾´é‡é–‹ç™º	æ§˜ã€…ãªç‰¹å¾´é‡ã‚’è©¦ã™
Week 5-8	ãƒ¢ãƒ‡ãƒ«æ”¹å–„	è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
Week 9-11	ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«	ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›
Week 12	æœ€çµ‚èª¿æ•´	ã‚³ãƒ¼ãƒ‰æ•´ç†ã€æœ€çµ‚æå‡ºé¸æŠ
ğŸ’¡ åˆå¿ƒè€…å‘ã‘Tips
âœ… ã‚„ã‚‹ã¹ãã“ã¨
ã¾ãšæå‡ºã™ã‚‹: å®Œç’§ã‚’ç›®æŒ‡ã•ãšã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ã™ãæå‡º
CVã‚’ä¿¡é ¼: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’é‡è¦–
Discussion/Notebookã‚’è¦‹ã‚‹: ä»–ã®å‚åŠ è€…ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å­¦ã¶
å°ã•ãªæ”¹å–„ã‚’ç©ã¿é‡ã­ã‚‹: ä¸€æ°—ã«å®Œç’§ã‚’ç›®æŒ‡ã•ãªã„
âŒ é¿ã‘ã‚‹ã¹ãã“ã¨
éåº¦ãªè¤‡é›‘åŒ–: åˆæœŸã‹ã‚‰è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã‚ãªã„
Public LBéå­¦ç¿’: Public LBã ã‘ã‚’è¿½ã„ã‹ã‘ãªã„
1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã«å›ºåŸ·: è¤‡æ•°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦ã™
ç· ã‚åˆ‡ã‚Šç›´å‰ã®æå‡º: ä½™è£•ã‚’æŒã£ã¦æå‡º
ğŸ“š ãŠã™ã™ã‚ã®åˆå¿ƒè€…å‘ã‘ã‚³ãƒ³ãƒš
Titanic - åˆ†é¡å•é¡Œã®å…¥é–€
House Prices - å›å¸°å•é¡Œã®å…¥é–€
Digit Recognizer - ç”»åƒåˆ†é¡ã®å…¥é–€
ã“ã‚Œã‚‰ã¯ã€ŒGetting Startedã€ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚„ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ãŒè±Šå¯Œã§ã™ã€‚
ã“ã®ã‚ˆã†ãªæµã‚Œã§Kaggleã‚³ãƒ³ãƒšã«å–ã‚Šçµ„ã¿ã¾ã™ã€‚è³ªå•ãŒã‚ã‚Œã°æ•™ãˆã¦ãã ã•ã„ï¼